{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LxSfnfUNx8Gz",
        "WR1D6etGyekT",
        "mda3xgvx0Die",
        "CFf36IrX0sGA",
        "J_GceXT2giOn",
        "D-J7MWzBgsJk",
        "q8SzrmjCgy32",
        "TqlnI10Mg-Hl",
        "yJsy8V2YhDzr",
        "HP3E0qnWqdMw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### <font color=yellow>CELL 0 : Import `logging`\n",
        "---\n",
        "THIS MUST BE FIRST TO WORK!"
      ],
      "metadata": {
        "id": "LxSfnfUNx8Gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LOGGING NEEDS TO BE THE FIRST IMPORT\n",
        "import logging                # For printing informative messages during execution\n",
        "import sys                    # For system-specific functions (like exiting)\n",
        "import datetime\n",
        "\n",
        "# Create a new log file for each run using a timestamp:\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "logging_filename = f'SheetExtractor_ExecutionLog_{timestamp}.log'\n",
        "file_mode = 'w'\n",
        "\n",
        "# ===================================================================================================\n",
        "# === Logging Setup ===\n",
        "# ===================================================================================================\n",
        "# 1. Get the root logger (or a specific logger)\n",
        "# Using root logger here to capture logs from module-level logging calls like logging.info()\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO) # Set the minimum level for the logger itself\n",
        "\n",
        "# 2. Clear existing handlers (important in notebooks, similar effect to force=True)\n",
        "if logger.hasHandlers():\n",
        "    logger.handlers.clear()\n",
        "\n",
        "# 3. Create a formatter\n",
        "log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s')\n",
        "\n",
        "# 4. Create a File Handler\n",
        "file_handler = logging.FileHandler(logging_filename, mode=file_mode)\n",
        "file_handler.setLevel(logging.INFO) # Set level for this handler\n",
        "file_handler.setFormatter(log_formatter)\n",
        "logger.addHandler(file_handler) # Add handler to the logger\n",
        "\n",
        "# 5. Create a Console/Stream Handler (Optional - for simultaneous console output)\n",
        "console_handler = logging.StreamHandler(sys.stdout) # Output to notebook cell\n",
        "console_handler.setLevel(logging.INFO) # Or set a different level for console if desired\n",
        "console_handler.setFormatter(log_formatter)\n",
        "logger.addHandler(console_handler)\n",
        "\n",
        "# Test the logging\n",
        "logging.info(f\"Logging initialized. Output to console and to: {logging_filename}\")\n",
        "\n",
        "\n",
        "# ===================================================================================================\n",
        "# === Logging Setup ===\n",
        "# ===================================================================================================\n",
        "# Configure basic logging, forcing it to overwrite previous handlers\n",
        "# logging.basicConfig(\n",
        "#     level=logging.INFO,\n",
        "#     format='%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s',\n",
        "#     force=True,  # <--- Add this\n",
        "#     stream=sys.stdout # <--- Explicitly direct to notebook stdout\n",
        "# )\n",
        "\n",
        "# # Test immediately after configuration\n",
        "# logging.info(\"Logging configured successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTPcfiofxkgT",
        "outputId": "6beae1ab-3398-4a1f-cb24-7da6b80b70c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-11 21:39:39,136 - INFO - [<cell line: 0>] - Logging configured successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### <font color=yellow>CELL 0.5 : Install Packages\n",
        "---\n",
        "This installs the basic software tools the script needs to work. It's like getting the right apps ready before you start.\n",
        "\n"
      ],
      "metadata": {
        "id": "WR1D6etGyekT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQTG_L4cyQXu",
        "outputId": "5eb0af03-2227-4998-f08f-3672979bf4b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required packages installed.\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages quietly (-q)\n",
        "%pip install -q pydantic Pillow google-genai\n",
        "\n",
        "print(\"Required packages installed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### <font color=yellow>CELL 1 : Import Libraries\n",
        "---\n",
        "This loads the specific commands and features the script will use. It's like opening the tools we need so they're ready to go."
      ],
      "metadata": {
        "id": "mda3xgvx0Die"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os                     # For interacting with the operating system (paths, directories)\n",
        "import shutil                 # For high-level file operations (finding tools, removing directories)\n",
        "import subprocess             # For running external command-line tools (Ghostscript, PDFtk)\n",
        "import tempfile               # For creating temporary files and directories\n",
        "import uuid                   # For generating unique IDs (for temporary folders)\n",
        "import glob                   # For finding files matching a pattern (like images)\n",
        "import re                     # For regular expressions (used in sorting image names)\n",
        "import time                   # For timing operations\n",
        "import json                   # For working with JSON data (AI response)\n",
        "from pathlib import Path      # For easier path manipulation (recommended over os.path)\n",
        "from typing import List, Dict, Any # For type hinting (improves code clarity)\n",
        "\n",
        "# Image handling\n",
        "from PIL import Image         # Pillow library for image operations\n",
        "\n",
        "# Google AI / Gemini\n",
        "from google import genai\n",
        "from google.genai import types as genai_types # Use alias to avoid conflict with 'types' module\n",
        "\n",
        "# Pydantic for data validation\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Colab specific for secrets, Drive, and file uploads\n",
        "try:\n",
        "    from google.colab import drive, userdata, files\n",
        "    COLAB_ENV = True\n",
        "    drive.mount(\"/content/drive\") # Mount Google Drive\n",
        "except ImportError:\n",
        "    print(\"WARNING: Not running in Colab or Colab modules failed to import. File upload/Drive access may fail.\")\n",
        "    COLAB_ENV = False\n",
        "    # Define dummy variables if not in Colab, so later checks don't fail\n",
        "    userdata = None\n",
        "    files = None\n",
        "\n",
        "# ===================================================================================================\n",
        "# === Pydantic Models (Expected JSON Structure) ===\n",
        "# ===================================================================================================\n",
        "# Define the structure Gemini should return\n",
        "# ===================================================================================================\n",
        "class PageDetail(BaseModel):\n",
        "    page_number: int = Field(description=\"The page number in the document where this information was found.\")\n",
        "    sheet_number: str = Field(description=\"The sheet number extracted from the title block of the page (e.g., 'CS', 'C2.1', 'A-101').\")\n",
        "    sheet_title: str = Field(description=\"The sheet title, in Title Case, and sanitized using the provided sanitization rules. Extracted from the title block of the page. For example: 'Demolition Plan', 'Cover Sheet', 'Sesc Plan 1 of 6'.\")\n",
        "\n",
        "class ExtractedData(BaseModel):\n",
        "    total_num_pages_all_parts: int = Field(description=\"The total number of pages represented by the input images.\")\n",
        "    pages: List[PageDetail] = Field(description=\"A list of bookmark entries, one for each page/image processed, ORDERED by page_number.\")\n",
        "\n",
        "print(\"Libraries imported & Pydantic strcuture defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wsd3g_750D7D",
        "outputId": "eab10dd5-6434-41eb-fc23-b020968fefa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Libraries imported & Pydantic strcuture defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### <font color=yellow>CELL 2 : Configuration and Logging Setup\n",
        "---\n",
        "This sets up the main settings, like where to save the final file and the instructions for the AI. It also securely handles the AI password."
      ],
      "metadata": {
        "id": "CFf36IrX0sGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AppConfig:\n",
        "    \"\"\"Holds all configuration settings for the application.\"\"\"\n",
        "# ===================================================================================================\n",
        "# === 1. Output Settings ===\n",
        "# ===================================================================================================\n",
        "    # Base directory on Google Drive where the final bookmarked PDF and temp folders will be created.\n",
        "    BASE_OUTPUT_DIR: str = \"/content/drive/MyDrive/Apr11-Sheet-Extraction/\"\n",
        "\n",
        "# ===================================================================================================\n",
        "# === 2. Gemini API Settings ===\n",
        "# === DO NOT CHANGE THESE!\n",
        "# ===================================================================================================\n",
        "    # Attempt to get API key from Colab secrets first, then environment variable\n",
        "    GEMINI_API_KEY: str | None = None\n",
        "    if COLAB_ENV and userdata:\n",
        "        GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    if not GEMINI_API_KEY:\n",
        "         GEMINI_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "    # Add a default placeholder if you want, but raising an error later if None is safer\n",
        "    # if not GEMINI_API_KEY:\n",
        "    #     GEMINI_API_KEY = \"YOUR_API_KEY_HERE\" # Replace if not using secrets/env vars\n",
        "\n",
        "    GEMINI_MODEL_NAME: str = \"gemini-2.5-pro-exp-03-25\"\n",
        "    GEMINI_TEMPERATURE: float = 0.0\n",
        "\n",
        "# ===================================================================================================\n",
        "# === 3. Ghostscript Settings ===\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# === NOTE: GS_RESOLUTION value controls the resolution of the images created from the PDF.         #\n",
        "# === DON'T SET GS_RESOLUTION UNDER 80                                                              #\n",
        "    GS_RESOLUTION: int = 120                                                                        #\n",
        "# ---------------------------------------------------------------------------------------------------\n",
        "# === NOTE: GS_IMAGE_FORMAT value should be left as 'pnggray'                                       #\n",
        "    GS_IMAGE_FORMAT: str = 'pnggray'                                                                #\n",
        "# ===================================================================================================\n",
        "\n",
        "# ===================================================================================================\n",
        "# === 4. System Prompt for Gemini (aka System Instructions) ===\n",
        "# ===================================================================================================\n",
        "    GEMINI_SYSTEM_PROMPT_TEMPLATE: str = \"\"\"<ROLE>\n",
        "\n",
        "**You are an expert at finding and extracting** a `sheet_number` (aka sheet no. or plan no., etc.) and a `sheet_title` **from within the 'Title Block' of each page** (each page being an image) of a construction drawings/plans document.\n",
        "    * You must first identify the 'Title Block' area.\n",
        "    * DO NOT use data from the center of the page. **This is not the 'Title Block'**.\n",
        "\n",
        "</ROLE>\n",
        "\n",
        "<CONTEXT>\n",
        "\n",
        "* You are processing **exactly {actual_total_pages} pages of a construction drawings document** that has been converted to {actual_total_pages} single-page images.\n",
        "    * The images are provided sequentially below.\n",
        "* Note: While the 'Title Block' contains the required information, its exact visual layout, positioning of elements, or surrounding\n",
        "text might vary in different parts of the document set. As a hypothetical example:\n",
        "    * Pgs1-49 might use 'Title_Block_Layout_1', while Pgs 50-100 might use 'Title_Block_Layout_2', etc.\n",
        "    * **Title Blocks are always found on the bottom edge or right-side edge of each page**, with **`sheet_number` and `sheet_title` generally appearing along the\n",
        "right-side edge or in the lower-right corner**.\n",
        "    * **DO NOT use data from the top or center of the page** even if you believe it's the `sheet_title` as this data is incorrect.\n",
        "    * Focus on identifying the semantic meaning of `sheet_number` and `sheet_title` within the title block area, regardless of minor layout shifts. Including:\n",
        "        * Patterns in the structure of the `sheet_number` and how it relates to the corresponding `sheet_title`.\n",
        "\n",
        "</CONTEXT>\n",
        "\n",
        "<TERM_DEFINITIONS>\n",
        "\n",
        "(FYI: Terms can vary slightly in these documents!)\n",
        "\n",
        "1. `sheet_number` (aka 'Sheet No.' or similar) is the plan or drawing number and **found in the title block** on each page (image).\n",
        "    * `sheet_number` hypothetical format/structure examples:\n",
        "        * 'A1.1'\n",
        "        * 'M2.0S1'\n",
        "        * 'M-304'\n",
        "        * 'S200'\n",
        "        * 'A0-CS2'\n",
        "        * 'BA-101'\n",
        "        * There are many other alphanumerical (including decimals and hyphens) combinations!\n",
        "2. `sheet_title` (aka 'Plan Title', 'Drawing Title', or similar) is the name of each Drawing/Plan and is **found in the title block** on each page (image).\n",
        "    * `sheet_title` hypothetical format/structure examples:\n",
        "        * '2nd Floor Plan - Area A'\n",
        "        * '1st Floor Reflected Ceiling Plan'\n",
        "        * 'Mechanical Schedules'\n",
        "        * 'Plan and Profile'\n",
        "        * More examples below!\n",
        "    * ALWAYS sanitize `sheet_title` data using a combination of the following '**SANITIZATION RULES (for `sheet_title` JSON output)**':\n",
        "        1. Symbols & Punctuation:\n",
        "            - '-' is the only symbol allowed in the final `sheet_title` data in the JSON output.\n",
        "            - '/' and '\\\\' in the document, become '-' in the JSON output.\n",
        "            - '&' in the document, becomes 'and' in the JSON output.\n",
        "            - ',', '.', and '#' in the document, are removed in the JSON output.\n",
        "            - '(', ')', '[', ']', '{{', and '}}' in the document, are removed in the JSON output while keeping the enclosed text.\n",
        "        2. VERY IMPORTANT (to avoid failure):\n",
        "            - **ALWAYS convert `sheet_title` to 'Title Case'**.\n",
        "            - **Never output `sheet_title` in ALL CAPS**.\n",
        "        3. Using pattern \"*appearance in document* :: **sanitized data for JSON output**\" here are some real-world sanitization examples:\n",
        "            - *Detached Garage #2 & #3 Enlarged Electrical Plans* :: **Detached Garage 2 and 3 Enlarged Electrical Plans**\n",
        "            - *1st, 2nd, & 3rd Floor Bldg. Plans / Notes* :: **1st 2nd and 3rd Floor Bldg Plans - Notes**\n",
        "            - *Grading and SESC Plan (1 of 6)* :: **Grading and Sesc Plan 1 of 6**\n",
        "            - *3rd FLOOR BUILDING PLANS* :: **3rd Floor Building Plans**\n",
        "            - *1st Floor Partial Bldg. Plans - Units \\\\\\\"D8-H\\\\\\\" & \\\\\\\"T1-H\\\\\\\"* :: **1st Floor Partial Bldg Plans - Units D8-H and T1-H**\n",
        "\n",
        "</TERM_DEFINITIONS>\n",
        "\n",
        "<TASK>\n",
        "1. You will analyze the provided pages (images) page by page (image by image), sequentially, covering all {actual_total_pages} pages.\n",
        "2. For each page (image), determine the corresponding `page_number` (starting from 1 for the first image and incrementing sequentially up to {actual_total_pages} ).\n",
        "3. Extract the Sheet Number and Sheet Title by visually reading the TITLE BLOCK within each page (image), disregard most if not all of the body of the pages.\n",
        "4. Your output **MUST** be a single JSON **object** conforming to the specified response schema.\n",
        "5. This JSON object must contain:\n",
        "    * `total_num_pages_all_parts`: The total number of pages. This **MUST** equal {actual_total_pages}.\n",
        "    * `pages`: A JSON array containing exactly {actual_total_pages} objects, one for each page/image processed.\n",
        "        * Each object in this array must contain the `page_number`, `sheet_number`, and `sheet_title` for that specific page/image.\n",
        "</TASK>\n",
        "\n",
        "<KEYS_TO_SUCCESS>\n",
        "\n",
        "These are your KEYS TO SUCCESS:\n",
        "1. **NEVER make assumptions**\n",
        "2. Every sheet number/title in your output JSON will use **REAL DATA that you visually read/extracted from the page images**\n",
        "3. Do not include any bullet points, numbering, or extra commentary outside the JSON structure.\n",
        "4. The `pages` array **MUST** contain exactly {actual_total_pages} entries, ordered sequentially by `page_number` starting from 1 and ending at {actual_total_pages}.\n",
        "5. Ensure the value for `total_num_pages_all_parts` **MUST** be exactly {actual_total_pages}.\n",
        "6. The data will be consistently located from page to page in most cases aside from cover/title sheets and oddball documents.\n",
        "7. The first page (`page_number` = 1), if it's a cover/title sheet or list of drawings, will always have `sheet_number` \"CS\" or \"TS\", and `sheet_title` of \"Cover Sheet\" or \"Title Sheet\".\n",
        "8. Always output \"pretty-printed\" human-readable JSON.\n",
        "\n",
        "</KEYS_TO_SUCCESS>\"\"\"\n",
        "\n",
        "# ===================================================================================================\n",
        "# === 6. Create a Config Instance ===\n",
        "# ===================================================================================================\n",
        "# ‼️ We will use this 'config' object throughout the script to access settings ‼️\n",
        "try:\n",
        "    config = AppConfig()\n",
        "    # Validate essential config like API key immediately\n",
        "    if not config.GEMINI_API_KEY:\n",
        "        raise ValueError(\"Gemini API Key not found in Colab secrets or environment variables.\")\n",
        "    logging.info(\"Application configuration loaded successfully.\")\n",
        "except Exception as e:\n",
        "    logging.critical(f\"Failed to initialize configuration: {e}\")\n",
        "    # Optionally exit if config is invalid\n",
        "    # sys.exit(f\"CRITICAL ERROR: Configuration failed - {e}\")\n",
        "\n",
        "print(\"Configuration class defined and instance created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-qNRFqg02mC",
        "outputId": "71b3c051-7ca8-4e52-cbbb-0d320804f817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-11 21:40:22,898 - INFO - [<cell line: 0>] - Application configuration loaded successfully.\n",
            "Configuration class defined and instance created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### <font color=yellow>CELL 3: External Tool Finder Function\n",
        "---\n",
        "This finds essential helper programs the script needs. If they're missing, it tries to install them automatically."
      ],
      "metadata": {
        "id": "J_GceXT2giOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_executable(tool_name: str, common_names: List[str]) -> str:\n",
        "    \"\"\"\n",
        "    Finds the executable path for a given tool using common names and the system PATH.\n",
        "    Attempts installation via apt-get on Colab/Linux if not initially found.\n",
        "\n",
        "    Args:\n",
        "        tool_name: The generic name of the tool (for logging, e.g., \"Ghostscript\").\n",
        "        common_names: A list of possible executable names (e.g., [\"gs\", \"gswin64c\"]).\n",
        "\n",
        "    Returns:\n",
        "        The absolute path to the first found executable.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If none of the common names for the executable can be found\n",
        "                           in the system PATH, even after an installation attempt.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Searching for {tool_name} executable (checking names: {common_names})...\")\n",
        "    for name in common_names:\n",
        "        path = shutil.which(name)\n",
        "        if path:\n",
        "            logging.info(f\"{tool_name} found as '{name}' at: {path}\")\n",
        "            return path # Return the path as soon as one is found\n",
        "\n",
        "    # If the loop finishes without finding the tool initially\n",
        "    logging.warning(f\"{tool_name} not found in PATH using names: {common_names}.\")\n",
        "    error_msg = f\"Required executable '{tool_name}' not found in system PATH using names: {common_names}.\" # Default error\n",
        "\n",
        "    # Attempt installation if in Colab/Linux (optional, based on original script)\n",
        "    install_attempted = False\n",
        "    if COLAB_ENV and sys.platform.startswith(\"linux\"):\n",
        "        install_package = None\n",
        "        if tool_name == \"Ghostscript\":\n",
        "            install_package = \"ghostscript\"\n",
        "        elif tool_name == \"PDFtk\":\n",
        "            # Try pdftk-java first as it's common on newer systems\n",
        "            install_package = \"pdftk-java\" # or just \"pdftk\" if that fails\n",
        "\n",
        "        if install_package:\n",
        "            install_attempted = True\n",
        "            logging.warning(f\"Attempting to install {tool_name} ({install_package}) via apt-get...\")\n",
        "            try:\n",
        "                # Run update quietly first\n",
        "                subprocess.run(['apt-get', 'update', '-qq'], check=True, capture_output=True, timeout=520)\n",
        "                # Run install quietly\n",
        "                subprocess.run(['apt-get', 'install', '-y', '-qq', install_package], check=True, capture_output=True, timeout=580)\n",
        "                logging.info(f\"Installation command for {install_package} completed.\")\n",
        "                # Check again after installation\n",
        "                # For PDFtk, the command is usually just 'pdftk' regardless of package name\n",
        "                check_name = \"pdftk\" if tool_name == \"PDFtk\" else common_names[0] # Check primary name\n",
        "                path_after_install = shutil.which(check_name)\n",
        "                if path_after_install:\n",
        "                    logging.info(f\"{tool_name} installed and found at: {path_after_install}\")\n",
        "                    return path_after_install # Success! Return the path\n",
        "                else:\n",
        "                     # If pdftk-java install didn't make 'pdftk' available, maybe try 'pdftk' package?\n",
        "                     # (Add logic here if needed for fallback package install)\n",
        "                     error_msg = f\"{tool_name} installation attempted ({install_package}), but executable '{check_name}' still not found.\"\n",
        "                     logging.error(error_msg)\n",
        "\n",
        "            except subprocess.TimeoutExpired as time_e:\n",
        "                 logging.error(f\"Timeout during {tool_name} installation attempt: {time_e}\")\n",
        "                 error_msg = f\"Timeout occurred while trying to install {tool_name}.\"\n",
        "            except subprocess.CalledProcessError as install_e:\n",
        "                logging.error(f\"apt-get command failed during {tool_name} installation attempt: {install_e}\")\n",
        "                error_msg = f\"Installation command failed for {tool_name}.\"\n",
        "            except Exception as install_e:\n",
        "                logging.error(f\"Unexpected error during {tool_name} installation attempt: {install_e}\")\n",
        "                error_msg = f\"An unexpected error occurred during {tool_name} installation.\"\n",
        "\n",
        "    # If not found initially and either not Colab/Linux or install failed/didn't find it\n",
        "    logging.error(error_msg) # Log the final error message\n",
        "    raise FileNotFoundError(error_msg) # Raise the exception\n",
        "\n",
        "print(\"External tool finder function 'find_executable' defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7BAvEgegpm0",
        "outputId": "d7d69cd0-1ef3-4fc4-fdae-d3f12460fd78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "External tool finder function 'find_executable' defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### <font color=yellow>CELL 4: Path Management Function\n",
        "---\n",
        "This creates a unique temporary folder for each PDF to keep working files organized. It also decides where the final bookmarked PDF will be saved."
      ],
      "metadata": {
        "id": "D-J7MWzBgsJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_processing_paths(output_base_dir: str, original_filename: str) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Generates and creates a unique set of paths for processing a single file.\n",
        "\n",
        "    Creates a job-specific temporary directory within the base output directory\n",
        "    to isolate intermediate files (images, JSON response, bookmark data).\n",
        "    The final bookmarked PDF is saved directly in the base output directory.\n",
        "\n",
        "    Args:\n",
        "        output_base_dir: The base directory from AppConfig where outputs are stored.\n",
        "        original_filename: The original name of the uploaded file.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing string paths for various stages:\n",
        "        'temp_dir': Path to the unique temporary directory for this job's intermediates.\n",
        "        'image_output_dir': Subdirectory within temp_dir for generated images.\n",
        "        'response_file_path': Path within temp_dir for storing the raw AI response JSON.\n",
        "        'bookmark_file_path': Path within temp_dir for the generated PDFtk bookmark file.\n",
        "        'final_output_path': Path for the final bookmarked PDF (in base_output_dir).\n",
        "\n",
        "    Raises:\n",
        "        OSError: If directory creation fails.\n",
        "    \"\"\"\n",
        "    # Create a 'safe' version of the filename stem for use in directory names\n",
        "    # Replace spaces and remove extension\n",
        "    safe_base_filename = Path(original_filename).stem.replace(\" \", \"_\").replace(\".\", \"_\")\n",
        "    # Generate a short unique ID to prevent collisions even if processing the same file multiple times\n",
        "    unique_id = uuid.uuid4().hex[:8]\n",
        "\n",
        "    # Define the unique temporary directory for this processing job's intermediate files\n",
        "    # Place it inside the main output directory for better organization during runs\n",
        "    temp_dir = Path(output_base_dir) / f\"processing_{safe_base_filename}_{unique_id}\"\n",
        "\n",
        "    # Define paths for intermediate files within the temporary directory\n",
        "    image_output_dir = temp_dir / \"images\"\n",
        "    response_file_path = temp_dir / f\"{safe_base_filename}_response.json\"\n",
        "    bookmark_file_path = temp_dir / f\"{safe_base_filename}_bookmarks.txt\"\n",
        "\n",
        "    # Define the final output path (saved directly in the base output dir, not temp)\n",
        "    # Prepend \"Autobookmarked_\" to easily identify the final product\n",
        "    final_output_filename = f\"Autobookmarked_{original_filename}\"\n",
        "    final_output_path = Path(output_base_dir) / final_output_filename\n",
        "\n",
        "    # Create the temporary directory and its image subdirectory\n",
        "    try:\n",
        "        # exist_ok=True prevents error if it somehow already exists (unlikely with UUID)\n",
        "        os.makedirs(image_output_dir, exist_ok=True)\n",
        "        # Also ensure the main output directory exists (important for final_output_path)\n",
        "        os.makedirs(output_base_dir, exist_ok=True)\n",
        "        logging.info(f\"Created temporary processing directory for intermediates: {temp_dir}\")\n",
        "    except OSError as e:\n",
        "        logging.error(f\"Failed to create processing directories under {temp_dir}: {e}\")\n",
        "        raise # Re-raise the error as this is critical for the workflow\n",
        "\n",
        "    # Store paths as strings in the dictionary (compatible with older functions/libraries)\n",
        "    paths = {\n",
        "        \"temp_dir\": str(temp_dir),\n",
        "        \"image_output_dir\": str(image_output_dir),\n",
        "        \"response_file_path\": str(response_file_path),\n",
        "        \"bookmark_file_path\": str(bookmark_file_path),\n",
        "        \"final_output_path\": str(final_output_path),\n",
        "    }\n",
        "    logging.info(f\"Generated processing paths for '{original_filename}'.\")\n",
        "    return paths\n",
        "\n",
        "print(\"Path management function 'get_processing_paths' defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD-7Lv7xgv-o",
        "outputId": "8809c947-b9aa-439d-b51d-8d72b243dda7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path management function 'get_processing_paths' defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### <font color=yellow>CELL 5: Core Processing Functions (Parameterized)\n",
        "---\n",
        "This defines the main steps: converting PDF pages to images, asking the AI to read them, formatting the AI's answer into bookmarks, and adding those bookmarks to the PDF."
      ],
      "metadata": {
        "id": "q8SzrmjCgy32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. PDF to Images Function ---\n",
        "def pdf_to_images(pdf_path: str, output_dir: str, gs_path: str, resolution: int, image_format: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Converts PDF pages to images using Ghostscript. (Adapted from original)\n",
        "\n",
        "    Args:\n",
        "        pdf_path: Path to the input PDF file.\n",
        "        output_dir: Directory where output images will be saved (should exist).\n",
        "        gs_path: Path to the Ghostscript executable.\n",
        "        resolution: Output resolution in DPI.\n",
        "        image_format: Ghostscript device format string (e.g., 'pnggray').\n",
        "\n",
        "    Returns:\n",
        "        A sorted list of absolute paths to the generated image files.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the input PDF doesn't exist.\n",
        "        ValueError: If gs_path is not provided.\n",
        "        RuntimeError: If Ghostscript fails, times out, or produces no images.\n",
        "        subprocess.CalledProcessError: If Ghostscript returns a non-zero exit code.\n",
        "        subprocess.TimeoutExpired: If Ghostscript takes too long.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Attempting PDF to image conversion for: {os.path.basename(pdf_path)}\")\n",
        "    if not os.path.exists(pdf_path):\n",
        "         raise FileNotFoundError(f\"Input PDF not found at: {pdf_path}\")\n",
        "    if not gs_path:\n",
        "         raise ValueError(\"Ghostscript path is not provided.\")\n",
        "\n",
        "    # Determine file extension based on format\n",
        "    if 'png' in image_format: ext = 'png'\n",
        "    elif 'jpeg' in image_format or 'jpg' in image_format: ext = 'jpg'\n",
        "    else: ext = 'img' # Fallback\n",
        "\n",
        "    # Define the output pattern within the specific output_dir for this job\n",
        "    output_pattern = os.path.join(output_dir, f\"page_%04d.{ext}\")\n",
        "\n",
        "    # Construct the command\n",
        "    command = [\n",
        "        gs_path,\n",
        "        \"-dNOPAUSE\",          # Don't pause between pages\n",
        "        \"-dBATCH\",            # Exit after processing\n",
        "        \"-dSAFER\",            # Run in sandbox mode\n",
        "        \"-q\",                 # Suppress informational messages\n",
        "        f\"-sDEVICE={image_format}\", # Set output format\n",
        "        f\"-r{resolution}\",     # Set resolution\n",
        "        f\"-sOutputFile={output_pattern}\", # Set output file pattern\n",
        "        pdf_path              # Input PDF file\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        logging.info(f\"Running Ghostscript command: {' '.join(command)}\")\n",
        "        # Execute with a timeout (e.g., 5 minutes = 300 seconds)\n",
        "        result = subprocess.run(command, capture_output=True, check=True, timeout=1200)\n",
        "        # Log any stderr output from Ghostscript (often contains warnings or info)\n",
        "        stderr_output = result.stderr.decode(errors='ignore')\n",
        "        if stderr_output and stderr_output.strip():\n",
        "             logging.debug(f\"Ghostscript stderr: {stderr_output.strip()}\")\n",
        "        logging.info(\"Ghostscript conversion command executed successfully.\")\n",
        "\n",
        "        # Find the generated image files\n",
        "        glob_pattern = os.path.join(output_dir, f\"page_*.{ext}\")\n",
        "        image_files = glob.glob(glob_pattern)\n",
        "        if not image_files:\n",
        "            # This is an error condition - GS ran but produced nothing\n",
        "            raise RuntimeError(f\"Ghostscript ran successfully but no images found matching pattern '{glob_pattern}'. Check GS logs or input PDF.\")\n",
        "\n",
        "        # Sort the files numerically based on the page number in the filename\n",
        "        def sort_key(filepath):\n",
        "            # Use regex to extract the number part of 'page_NNNN.ext'\n",
        "            match = re.search(r'page_(\\d+)\\.' + re.escape(ext) + '$', os.path.basename(filepath))\n",
        "            # Return the integer page number if found, otherwise -1 for robust sorting\n",
        "            return int(match.group(1)) if match else -1\n",
        "        image_files.sort(key=sort_key)\n",
        "\n",
        "        logging.info(f\"Found and sorted {len(image_files)} image file(s) in {output_dir}.\")\n",
        "        return image_files # Return the sorted list of paths\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle errors where Ghostscript itself reported failure (non-zero exit code)\n",
        "        stderr = e.stderr.decode(errors='ignore') if e.stderr else 'N/A'\n",
        "        stdout = e.stdout.decode(errors='ignore') if e.stdout else 'N/A'\n",
        "        logging.error(f\"Ghostscript failed. Command: '{e.cmd}'. Return code: {e.returncode}. Stderr: {stderr}\")\n",
        "        # Raise a more specific error indicating the source\n",
        "        raise RuntimeError(f\"Ghostscript conversion failed for {os.path.basename(pdf_path)}\") from e\n",
        "    except subprocess.TimeoutExpired:\n",
        "         # Handle cases where Ghostscript took too long\n",
        "         logging.error(f\"Ghostscript command timed out after 300 seconds for {os.path.basename(pdf_path)}.\")\n",
        "         raise RuntimeError(f\"Ghostscript timed out for {os.path.basename(pdf_path)}\")\n",
        "    except Exception as e:\n",
        "        # Catch any other unexpected errors during the process\n",
        "        logging.error(f\"Unexpected error during Ghostscript conversion: {e}\", exc_info=True)\n",
        "        raise RuntimeError(f\"Unexpected error during image conversion for {os.path.basename(pdf_path)}\") from e\n",
        "\n",
        "# --- 2. Generate Bookmarks Function ---\n",
        "def generate_bookmarks_ai(client: genai.Client, model_name: str, temperature: float, system_prompt_template: str, image_paths: List[str], response_schema: BaseModel) -> Dict:\n",
        "    \"\"\"\n",
        "    Generates bookmark data from images using the Gemini API. (Adapted from original)\n",
        "\n",
        "    Args:\n",
        "        client: Initialized Gemini API client instance.\n",
        "        model_name: Name of the Gemini model to use.\n",
        "        temperature: Temperature setting for the AI model.\n",
        "        system_prompt_template: The template string for the system prompt,\n",
        "                                containing '{actual_total_pages}'.\n",
        "        image_paths: A list of paths to the image files (sorted).\n",
        "        response_schema: The Pydantic model class defining the expected JSON structure.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary representing the parsed JSON response from the AI.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If client or image_paths are invalid.\n",
        "        RuntimeError: If image preparation, API call, or response parsing fails.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Starting AI bookmark generation for {len(image_paths)} images.\")\n",
        "    if not client: raise ValueError(\"Gemini client is not provided.\")\n",
        "    if not image_paths: raise ValueError(\"No image paths provided for AI generation.\")\n",
        "\n",
        "    actual_total_pages = len(image_paths)\n",
        "    image_parts = [] # List to hold prepared image data for the API\n",
        "\n",
        "    # Prepare image parts (read bytes, determine MIME type)\n",
        "    try:\n",
        "        for i, path in enumerate(image_paths):\n",
        "            if not os.path.exists(path):\n",
        "                 logging.warning(f\"Image file not found: {path}. Skipping.\")\n",
        "                 continue # Skip missing images\n",
        "\n",
        "            mime_type = None\n",
        "            try:\n",
        "                # Use Pillow to reliably get the MIME type\n",
        "                with Image.open(path) as img:\n",
        "                    # Convert format (like 'PNG') to MIME ('image/png')\n",
        "                    mime_type = Image.MIME.get(img.format.upper())\n",
        "                    if not mime_type: # Fallback based on file extension if Pillow fails\n",
        "                         ext = os.path.splitext(path)[1].lower()\n",
        "                         if ext == \".png\": mime_type = \"image/png\"\n",
        "                         elif ext in [\".jpg\", \".jpeg\"]: mime_type = \"image/jpeg\"\n",
        "                         elif ext == \".webp\": mime_type = \"image/webp\" # Add other supported types if needed\n",
        "                         else: raise ValueError(f\"Cannot determine MIME type for {path}\")\n",
        "            except Exception as img_e:\n",
        "                logging.warning(f\"Could not determine MIME type for {path}: {img_e}. Skipping file.\")\n",
        "                continue # Skip this image if we can't determine type\n",
        "\n",
        "            # Read the image file as binary data\n",
        "            with open(path, 'rb') as f: img_bytes = f.read()\n",
        "            # Create a Gemini API 'Part' object from the bytes and MIME type\n",
        "            image_parts.append(genai_types.Part.from_bytes(data=img_bytes, mime_type=mime_type))\n",
        "\n",
        "        if not image_parts:\n",
        "             # This happens if all images failed MIME type detection or were missing\n",
        "             raise RuntimeError(\"No valid image parts could be prepared for the API call.\")\n",
        "        logging.info(f\"Prepared {len(image_parts)} image parts for API.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error preparing image parts: {e}\", exc_info=True)\n",
        "        raise RuntimeError(\"Failed during image preparation for AI.\") from e\n",
        "\n",
        "        # Format the system prompt with the actual number of pages\n",
        "\n",
        "    try:\n",
        "        # This is where the formatting happens\n",
        "        formatted_system_prompt = system_prompt_template.format(actual_total_pages=actual_total_pages)\n",
        "    except KeyError as e:\n",
        "            logging.error(f\"System prompt template is missing '{e}' placeholder.\") # This is the error being logged\n",
        "            raise ValueError(\"Invalid system prompt template.\") from e\n",
        "\n",
        "    # Configure the Gemini API call\n",
        "    generate_content_config = genai_types.GenerateContentConfig(\n",
        "        temperature=temperature,\n",
        "        response_mime_type=\"application/json\", # Request JSON output\n",
        "        response_schema=response_schema, # Tell Gemini the expected structure (Pydantic model)\n",
        "        system_instruction=formatted_system_prompt,\n",
        "    )\n",
        "\n",
        "    # Make the API call\n",
        "    try:\n",
        "        logging.info(f\"Calling Gemini model '{model_name}' with {len(image_parts)} images...\")\n",
        "        # Add a timeout to the API request (e.g., 20 minutes = 1200 seconds)\n",
        "        response = client.models.generate_content(\n",
        "            model=model_name,\n",
        "            contents=image_parts, # Send the list of prepared image parts\n",
        "            config=generate_content_config\n",
        "        )\n",
        "\n",
        "        # Check if the response is valid and contains text\n",
        "        if not response or not hasattr(response, 'text') or not response.text:\n",
        "             # Log details if the response is empty or blocked\n",
        "             feedback = \"N/A\"\n",
        "             finish_reason = \"N/A\"\n",
        "             if response and hasattr(response, 'prompt_feedback'):\n",
        "                  feedback = response.prompt_feedback\n",
        "             if response and hasattr(response, 'candidates') and response.candidates:\n",
        "                  finish_reason = response.candidates[0].finish_reason\n",
        "             logging.error(f\"Gemini API call succeeded but returned no text. Feedback: {feedback}, Finish Reason: {finish_reason}\")\n",
        "             raise RuntimeError(\"Gemini API returned no text content. It might have been blocked or empty.\")\n",
        "\n",
        "        logging.info(\"Gemini API call successful, attempting to parse JSON response.\")\n",
        "\n",
        "        # --- Clean and Parse JSON ---\n",
        "        # Gemini might wrap the JSON in markdown fences (```json ... ```)\n",
        "        raw_text = response.text\n",
        "        # Find the first '{' and the last '}'\n",
        "        start_index = raw_text.find('{')\n",
        "        end_index = raw_text.rfind('}')\n",
        "\n",
        "        if start_index == -1 or end_index == -1 or end_index < start_index:\n",
        "             logging.error(f\"Could not find valid JSON object boundaries in response: {raw_text[:200]}...\") # Log beginning of text\n",
        "             raise ValueError(\"Invalid JSON structure received from Gemini API (missing '{' or '}').\")\n",
        "\n",
        "        # Extract the potential JSON string\n",
        "        json_core_string = raw_text[start_index : end_index + 1]\n",
        "\n",
        "        # Parse the extracted string into a Python dictionary\n",
        "        parsed_data = json.loads(json_core_string)\n",
        "        logging.info(\"Successfully parsed JSON response from Gemini.\")\n",
        "        # You could add validation against the Pydantic schema here if needed:\n",
        "        # try:\n",
        "        #     ExtractedData.model_validate(parsed_data)\n",
        "        #     logging.info(\"JSON response validated against Pydantic schema.\")\n",
        "        # except ValidationError as val_err:\n",
        "        #     logging.error(f\"JSON response failed Pydantic validation: {val_err}\")\n",
        "        #     raise ValueError(\"Gemini response did not match expected schema.\") from val_err\n",
        "        return parsed_data # Return the dictionary\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch errors from the API call or JSON parsing\n",
        "        logging.error(f\"Error during Gemini API call or response parsing: {e}\", exc_info=True)\n",
        "        # Check if it's a specific Gemini error type if needed\n",
        "        # if isinstance(e, genai_types.StopCandidateException):\n",
        "        #    logging.error(\"Gemini generation stopped potentially due to safety settings or other reasons.\")\n",
        "        raise RuntimeError(\"Gemini API interaction or response processing failed.\") from e\n",
        "\n",
        "# --- 3. Convert JSON to PDFtk Format Function ---\n",
        "def convert_ai_response_to_pdftk(response_data: Dict, output_bmk_path: str) -> bool:\n",
        "    \"\"\"\n",
        "    Converts the parsed AI response dictionary to PDFtk bookmark format text file.\n",
        "    (Adapted from original)\n",
        "\n",
        "    Args:\n",
        "        response_data: The dictionary parsed from the AI's JSON response.\n",
        "        output_bmk_path: The full path where the .txt bookmark file should be saved.\n",
        "\n",
        "    Returns:\n",
        "        True if conversion and saving were successful, False otherwise.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Converting AI response dictionary to PDFtk format for: {output_bmk_path}\")\n",
        "    try:\n",
        "        # Validate the structure of the response data\n",
        "        if 'pages' not in response_data or not isinstance(response_data['pages'], list):\n",
        "            logging.error(\"AI response JSON is missing the 'pages' list or it's not a list.\")\n",
        "            return False # Indicate failure\n",
        "\n",
        "        pages_data = response_data['pages']\n",
        "        if not pages_data:\n",
        "             logging.warning(\"AI response contained an empty 'pages' list. No bookmarks to generate.\")\n",
        "             # Write an empty file to indicate no bookmarks, maybe? Or return False?\n",
        "             # For now, let's treat it as success with no bookmarks.\n",
        "             with open(output_bmk_path, 'w', encoding='utf-8') as f:\n",
        "                 f.write(\"\") # Write empty file\n",
        "             return True\n",
        "\n",
        "\n",
        "        pdftk_bookmarks = [] # List to hold formatted bookmark strings\n",
        "        for page_detail in pages_data:\n",
        "            try:\n",
        "                # Extract required fields, handling potential missing keys gracefully\n",
        "                page_num = page_detail['page_number']\n",
        "                sheet_num = page_detail.get('sheet_number', 'MISSING_SHEET_NUM') # Use default if missing\n",
        "                sheet_title = page_detail.get('sheet_title', 'MISSING_SHEET_TITLE') # Use default if missing\n",
        "\n",
        "                # Combine sheet number and title for the bookmark text\n",
        "                bookmark_title = f\"{sheet_num} {sheet_title}\"\n",
        "\n",
        "                # Create the PDFtk bookmark entry string (ensure newline separation)\n",
        "                bookmark_entry = f\"BookmarkBegin\\nBookmarkTitle: {bookmark_title}\\nBookmarkLevel: 1\\nBookmarkPageNumber: {page_num}\\n\\n\"\n",
        "                pdftk_bookmarks.append(bookmark_entry)\n",
        "            except KeyError as e:\n",
        "                # This happens if 'page_number' is missing (others have defaults)\n",
        "                logging.warning(f\"Skipping page entry due to missing required key 'page_number': {e}. Entry: {page_detail}\")\n",
        "                continue # Skip this malformed entry and proceed with the next\n",
        "            except Exception as entry_e:\n",
        "                 logging.warning(f\"Error processing page entry: {entry_e}. Entry: {page_detail}\")\n",
        "                 continue # Skip problematic entries\n",
        "\n",
        "        if not pdftk_bookmarks:\n",
        "             logging.warning(\"No valid bookmark entries could be generated from the 'pages' data.\")\n",
        "             # Write an empty file if no valid entries were processed\n",
        "             with open(output_bmk_path, 'w', encoding='utf-8') as f:\n",
        "                 f.write(\"\")\n",
        "             return True # Still technically successful conversion (of nothing)\n",
        "\n",
        "        # Join all formatted bookmark strings into one final string\n",
        "        final_output_string = \"\".join(pdftk_bookmarks)\n",
        "\n",
        "        # Write the final string to the output text file\n",
        "        # Ensure the directory exists (should be handled by get_processing_paths)\n",
        "        os.makedirs(os.path.dirname(output_bmk_path), exist_ok=True)\n",
        "        with open(output_bmk_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(final_output_string)\n",
        "\n",
        "        logging.info(f\"Successfully saved PDFtk bookmarks ({len(pdftk_bookmarks)} entries) to {output_bmk_path}\")\n",
        "        return True # Indicate success\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error converting AI response to PDFtk format: {e}\", exc_info=True)\n",
        "        return False # Indicate failure\n",
        "\n",
        "# --- 4. Apply PDFtk Bookmarks Function ---\n",
        "def apply_bookmarks(pdftk_path: str, input_pdf_path: str, bookmark_data_path: str, output_pdf_path: str) -> bool:\n",
        "    \"\"\"\n",
        "    Applies bookmarks from a data file to a PDF using pdftk update_info.\n",
        "    (Adapted from original)\n",
        "\n",
        "    Args:\n",
        "        pdftk_path: Path to the PDFtk executable.\n",
        "        input_pdf_path: Path to the *original* input PDF file (the one to add bookmarks to).\n",
        "        bookmark_data_path: Path to the text file containing PDFtk bookmark data.\n",
        "        output_pdf_path: Path where the output PDF with bookmarks will be saved.\n",
        "\n",
        "    Returns:\n",
        "        True if pdftk command executed successfully and output file exists, False otherwise.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If pdftk_path is not provided.\n",
        "        FileNotFoundError: If input_pdf_path or bookmark_data_path do not exist.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Applying bookmarks from {os.path.basename(bookmark_data_path)} to {os.path.basename(input_pdf_path)}\")\n",
        "    if not pdftk_path: raise ValueError(\"PDFtk path is not provided.\")\n",
        "    if not os.path.exists(input_pdf_path): raise FileNotFoundError(f\"Input PDF for bookmarking not found: {input_pdf_path}\")\n",
        "    if not os.path.exists(bookmark_data_path): raise FileNotFoundError(f\"Bookmark data file not found: {bookmark_data_path}\")\n",
        "\n",
        "    # Construct the PDFtk command\n",
        "    command = [\n",
        "        pdftk_path,\n",
        "        input_pdf_path,\n",
        "        \"update_info\",       # Command to update metadata/bookmarks\n",
        "        bookmark_data_path,  # The bookmark data file\n",
        "        \"output\",\n",
        "        output_pdf_path      # The final output file path\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        logging.info(f\"Running PDFtk command: {' '.join(command)}\")\n",
        "        # Execute with a timeout (e.g., 2 minutes = 120 seconds)\n",
        "        result = subprocess.run(command, capture_output=True, check=True, text=True, encoding='utf-8', errors='ignore', timeout=120)\n",
        "\n",
        "        # Log PDFtk output (often useful for debugging)\n",
        "        stderr_output = result.stderr\n",
        "        if stderr_output and stderr_output.strip(): logging.debug(f\"PDFtk stderr: {stderr_output.strip()}\")\n",
        "        stdout_output = result.stdout\n",
        "        if stdout_output and stdout_output.strip(): logging.debug(f\"PDFtk stdout: {stdout_output.strip()}\")\n",
        "\n",
        "        # Verify that the output file was actually created and is not empty\n",
        "        if os.path.exists(output_pdf_path) and os.path.getsize(output_pdf_path) > 0:\n",
        "            logging.info(f\"PDFtk bookmark application successful. Output saved to: {output_pdf_path}\")\n",
        "            return True # Indicate success\n",
        "        else:\n",
        "            # This is an error: command ran but didn't produce the expected output\n",
        "            logging.error(f\"PDFtk command ran but output file '{output_pdf_path}' was not created or is empty.\")\n",
        "            # Attempt to remove the empty file if it exists\n",
        "            if os.path.exists(output_pdf_path):\n",
        "                 try: os.remove(output_pdf_path)\n",
        "                 except OSError: pass # Ignore error if removal fails\n",
        "            return False # Indicate failure\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle errors where PDFtk itself reported failure\n",
        "        stderr = e.stderr if isinstance(e.stderr, str) else e.stderr.decode(errors='ignore') if e.stderr else 'N/A'\n",
        "        stdout = e.stdout if isinstance(e.stdout, str) else e.stdout.decode(errors='ignore') if e.stdout else 'N/A'\n",
        "        logging.error(f\"PDFtk failed. Command: '{e.cmd}'. Return code: {e.returncode}. Stderr: {stderr}\")\n",
        "        return False # Indicate failure\n",
        "    except subprocess.TimeoutExpired:\n",
        "         # Handle cases where PDFtk took too long\n",
        "         logging.error(f\"PDFtk command timed out after 120 seconds for {os.path.basename(input_pdf_path)}.\")\n",
        "         return False # Indicate failure\n",
        "    except Exception as e:\n",
        "        # Catch any other unexpected errors\n",
        "        logging.error(f\"Unexpected error running PDFtk: {e}\", exc_info=True)\n",
        "        return False # Indicate failure\n",
        "\n",
        "print(\"Core processing functions defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnQsjZR3g2XO",
        "outputId": "35d83c43-6548-41a2-9c69-ca6650516a05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Core processing functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### <font color=yellow>CELL 6: Workflow Orchestration Function\n",
        "---\n",
        "This manages the entire process for one PDF, running the steps from Cell 5 in order. It also cleans up temporary files when finished."
      ],
      "metadata": {
        "id": "TqlnI10Mg-Hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_pdf_workflow(input_pdf_path: str, paths: Dict[str, str], gs_path: str, pdftk_path: str, client: genai.Client, config: AppConfig) -> str:\n",
        "    \"\"\"\n",
        "    Orchestrates the PDF processing pipeline for a single file.\n",
        "\n",
        "    Handles the sequence: PDF->Images->AI->Bookmarks->Apply.\n",
        "    Ensures the temporary *processing* directory (containing images, json, bookmarks.txt)\n",
        "    is cleaned up using a try...finally block, regardless of success or failure.\n",
        "\n",
        "    Args:\n",
        "        input_pdf_path: Path to the input PDF file (likely the temporary uploaded file).\n",
        "        paths: Dictionary of paths generated by get_processing_paths.\n",
        "        gs_path: Path to Ghostscript executable.\n",
        "        pdftk_path: Path to PDFtk executable.\n",
        "        client: Initialized Gemini API client.\n",
        "        config: Application configuration object (AppConfig instance).\n",
        "\n",
        "    Returns:\n",
        "        Path (str) to the final bookmarked PDF file upon successful completion of all steps.\n",
        "\n",
        "    Raises:\n",
        "        Exception: Propagates exceptions from any failed step (e.g., RuntimeError,\n",
        "                   FileNotFoundError, ValueError from the core functions).\n",
        "                   The caller (handle_uploaded_pdf) should catch this.\n",
        "    \"\"\"\n",
        "    # Get the path to the temporary directory created for this job's intermediates\n",
        "    # We need this early so the 'finally' block can use it even if an error occurs before it's used\n",
        "    temp_dir_to_clean = paths['temp_dir']\n",
        "    logging.info(f\"Starting PDF processing workflow for: {os.path.basename(input_pdf_path)}\")\n",
        "    logging.info(f\"Intermediate files will be stored in: {temp_dir_to_clean}\")\n",
        "    logging.info(f\"Final output target: {paths['final_output_path']}\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Convert PDF to Images (using function from Cell 5)\n",
        "        # Images are saved in paths['image_output_dir'] within the temp_dir\n",
        "        image_paths = pdf_to_images(\n",
        "            pdf_path=input_pdf_path,\n",
        "            output_dir=paths['image_output_dir'],\n",
        "            gs_path=gs_path,\n",
        "            resolution=config.GS_RESOLUTION,\n",
        "            image_format=config.GS_IMAGE_FORMAT\n",
        "        )\n",
        "        # pdf_to_images raises error on failure, so no need to check return value here\n",
        "\n",
        "        # Step 2: Generate Bookmarks via AI (using function from Cell 5)\n",
        "        ai_response_dict = generate_bookmarks_ai(\n",
        "            client=client,\n",
        "            model_name=config.GEMINI_MODEL_NAME,\n",
        "            temperature=config.GEMINI_TEMPERATURE,\n",
        "            system_prompt_template=config.GEMINI_SYSTEM_PROMPT_TEMPLATE,\n",
        "            image_paths=image_paths,\n",
        "            response_schema=ExtractedData # Pass the Pydantic model class from config\n",
        "        )\n",
        "        # generate_bookmarks_ai raises error on failure\n",
        "\n",
        "        # Optional: Save raw AI response dictionary for debugging (within temp_dir)\n",
        "        try:\n",
        "             with open(paths['response_file_path'], 'w', encoding='utf-8') as f:\n",
        "                 json.dump(ai_response_dict, f, indent=2)\n",
        "             logging.info(f\"Saved raw AI response JSON to {paths['response_file_path']}\")\n",
        "        except Exception as write_err:\n",
        "             # Log as warning, don't fail the whole process if this write fails\n",
        "             logging.warning(f\"Could not write AI response file: {write_err}\")\n",
        "\n",
        "\n",
        "        # Step 3: Convert AI Response to PDFtk Format (using function from Cell 5)\n",
        "        # Saves the bookmark data to paths['bookmark_file_path'] within temp_dir\n",
        "        success = convert_ai_response_to_pdftk(\n",
        "            response_data=ai_response_dict,\n",
        "            output_bmk_path=paths['bookmark_file_path']\n",
        "        )\n",
        "        if not success:\n",
        "            # Raise an error if conversion fails\n",
        "            raise RuntimeError(\"Failed to convert AI response to PDFtk bookmark format.\")\n",
        "\n",
        "        # Step 4: Apply Bookmarks using PDFtk (using function from Cell 5)\n",
        "        # Reads the original input PDF and the bookmark file from temp_dir,\n",
        "        # writes the final output to paths['final_output_path'] (outside temp_dir)\n",
        "        success = apply_bookmarks(\n",
        "            pdftk_path=pdftk_path,\n",
        "            input_pdf_path=input_pdf_path, # Apply to the original input PDF\n",
        "            bookmark_data_path=paths['bookmark_file_path'],\n",
        "            output_pdf_path=paths['final_output_path']\n",
        "        )\n",
        "        if not success:\n",
        "            # Raise an error if applying bookmarks fails\n",
        "            raise RuntimeError(\"Failed to apply PDFtk bookmarks to the PDF.\")\n",
        "\n",
        "        # If all steps above succeeded without raising an error:\n",
        "        logging.info(f\"Workflow completed successfully for {os.path.basename(input_pdf_path)}.\")\n",
        "        # Return the path to the final bookmarked PDF\n",
        "        return paths['final_output_path']\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any exception raised by the steps above\n",
        "        logging.error(f\"PDF processing workflow failed for {os.path.basename(input_pdf_path)}.\")\n",
        "        logging.error(f\"Error details: {e}\", exc_info=False) # Log basic error, set exc_info=True for full traceback\n",
        "        # Re-raise the exception so the calling function (handle_uploaded_pdf) knows it failed\n",
        "        raise\n",
        "\n",
        "    finally:\n",
        "        # Step 5: Cleanup Temporary Directory - THIS BLOCK *ALWAYS* RUNS\n",
        "        logging.info(f\"Initiating cleanup for temporary directory: {temp_dir_to_clean}\")\n",
        "        if temp_dir_to_clean and os.path.exists(temp_dir_to_clean):\n",
        "            try:\n",
        "                # Recursively remove the entire temporary directory and its contents\n",
        "                shutil.rmtree(temp_dir_to_clean)\n",
        "                logging.info(f\"Successfully cleaned up temporary directory: {temp_dir_to_clean}\")\n",
        "            except OSError as e:\n",
        "                # Log errors during cleanup, but don't raise again\n",
        "                # to avoid masking the original error (if one occurred)\n",
        "                logging.error(f\"Error during temporary directory cleanup {temp_dir_to_clean}: {e}\")\n",
        "        else:\n",
        "             # Log if the directory doesn't exist (maybe it failed to create or was already cleaned)\n",
        "             logging.info(\"Temporary directory does not exist or was already cleaned up.\")\n",
        "\n",
        "print(\"Workflow orchestration function 'process_pdf_workflow' defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCTomp1fhABG",
        "outputId": "6e92513c-ba4b-4519-f532-897484688580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Workflow orchestration function 'process_pdf_workflow' defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### <font color=yellow>CELL 7: Upload Handler Function\n",
        "---\n",
        "This handles the uploaded PDF file. It sets up the workspace and then tells the manager (Cell 6) to start the bookmarking process."
      ],
      "metadata": {
        "id": "yJsy8V2YhDzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_uploaded_pdf(uploaded_pdf_path: str, config: AppConfig, gs_path: str, pdftk_path: str, client: genai.Client) -> str | None:\n",
        "    \"\"\"\n",
        "    Handles the end-to-end processing of a single uploaded PDF file.\n",
        "\n",
        "    This acts as the primary entry point called by the main execution block\n",
        "    (or potentially a UI backend). It coordinates path generation and\n",
        "    calls the main processing workflow function.\n",
        "\n",
        "    Args:\n",
        "        uploaded_pdf_path: Path to the temporarily saved uploaded PDF file.\n",
        "        config: The application configuration object (AppConfig instance).\n",
        "        gs_path: Path to the Ghostscript executable.\n",
        "        pdftk_path: Path to the PDFtk executable.\n",
        "        client: Initialized Gemini API client instance.\n",
        "\n",
        "    Returns:\n",
        "        The absolute path (str) to the final bookmarked PDF file upon success,\n",
        "        or None if any part of the processing workflow fails.\n",
        "    \"\"\"\n",
        "    original_filename = os.path.basename(uploaded_pdf_path)\n",
        "    logging.info(f\"--- Starting processing request for file: {original_filename} ---\")\n",
        "    paths = None # Initialize in case path generation fails\n",
        "\n",
        "    try:\n",
        "        # 1. Generate unique paths for this processing job's intermediates and final output\n",
        "        # This creates the temporary directory structure needed by the workflow\n",
        "        paths = get_processing_paths(config.BASE_OUTPUT_DIR, original_filename)\n",
        "\n",
        "        # 2. Execute the core processing pipeline by calling the workflow orchestrator\n",
        "        # Pass the path to the uploaded file and all necessary tools/config\n",
        "        final_pdf_path = process_pdf_workflow(\n",
        "            input_pdf_path=uploaded_pdf_path, # Use the path of the uploaded file\n",
        "            paths=paths,                      # Pass the generated paths dictionary\n",
        "            gs_path=gs_path,\n",
        "            pdftk_path=pdftk_path,\n",
        "            client=client,\n",
        "            config=config\n",
        "        )\n",
        "\n",
        "        # If process_pdf_workflow completes without raising an exception, it was successful\n",
        "        logging.info(f\"Successfully processed '{original_filename}'.\")\n",
        "        return final_pdf_path # Return the path to the final bookmarked PDF\n",
        "\n",
        "    except Exception as e:\n",
        "        # If process_pdf_workflow (or get_processing_paths) raised an exception, catch it here.\n",
        "        # The error details should have already been logged within the workflow function.\n",
        "        logging.error(f\"Processing failed for '{original_filename}'. See previous logs for error details.\")\n",
        "        # The 'finally' block within process_pdf_workflow handles cleanup of the *processing* temp directory.\n",
        "        # Return None to indicate to the caller (__main__ block or UI) that processing failed.\n",
        "        return None\n",
        "    # Note: Cleanup of the *initial* uploaded_pdf_path itself (the one passed into this function)\n",
        "    # is the responsibility of the code that *calls* handle_uploaded_pdf (e.g., the __main__ block).\n",
        "\n",
        "print(\"Upload handler function 'handle_uploaded_pdf' defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLaTIkHbhFWz",
        "outputId": "765e5997-335e-4db3-bce1-c23b95c658b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload handler function 'handle_uploaded_pdf' defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### <font color=yellow>CELL 8: Main Execution Block (NEW STRATEGY)\n",
        "---\n",
        "The next three cells (#8, 9, and 10) act as the \"Start\" button for the whole script. It checks everything, asks for the PDF upload, runs the process using the handler (from Cell 7), and reports the final result."
      ],
      "metadata": {
        "id": "HP3E0qnWqdMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Setup Before Upload\n",
        "if __name__ == \"__main__\":\n",
        "    if not COLAB_ENV:\n",
        "        print(\"\\nERROR: Requires Colab environment.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    logging.info(\"=== Starting Main Execution Block ===\")\n",
        "    gs_path = None\n",
        "    pdftk_path = None\n",
        "    client = None\n",
        "    # We will get temp_upload_path in the next cell\n",
        "\n",
        "    try:\n",
        "        # --- 1. Load Config ---\n",
        "        logging.info(\"Using pre-loaded configuration.\")\n",
        "        if not config: raise RuntimeError(\"Configuration object 'config' not found.\")\n",
        "\n",
        "        # --- 2. Initialize Gemini Client ---\n",
        "        logging.info(\"Initializing Gemini client...\")\n",
        "        if not config.GEMINI_API_KEY: raise ValueError(\"Gemini API Key is missing.\")\n",
        "        client = genai.Client(api_key=config.GEMINI_API_KEY)\n",
        "        logging.info(\"Gemini client initialized successfully.\")\n",
        "\n",
        "        # --- 3. Find Essential External Tools ---\n",
        "        logging.info(\"Locating external tools (Ghostscript, PDFtk)...\")\n",
        "        gs_names = [\"gs\"] if sys.platform.startswith(\"linux\") else [\"gswin64c\", \"gswin32c\", \"gs\"]\n",
        "        pdftk_names = [\"pdftk\"]\n",
        "        gs_path = find_executable(\"Ghostscript\", gs_names)\n",
        "        pdftk_path = find_executable(\"PDFtk\", pdftk_names)\n",
        "        logging.info(\"External tools located successfully.\")\n",
        "\n",
        "        # --- Ready for Upload ---\n",
        "        logging.info(\"Setup complete. Proceed to the next cell to upload the PDF.\")\n",
        "\n",
        "    except Exception as setup_e:\n",
        "        # Catch setup errors here\n",
        "        logging.critical(f\"CRITICAL ERROR during setup: {setup_e}\", exc_info=True)\n",
        "        # Prevent proceeding if setup failed\n",
        "        raise RuntimeError(\"Setup failed, cannot proceed.\") from setup_e"
      ],
      "metadata": {
        "id": "LmDjQRPJqdMx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85bef2aa-e40c-4659-e134-38fadc39e75b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-11 21:40:23,047 - INFO - [<cell line: 0>] - === Starting Main Execution Block ===\n",
            "2025-04-11 21:40:23,050 - INFO - [<cell line: 0>] - Using pre-loaded configuration.\n",
            "2025-04-11 21:40:23,051 - INFO - [<cell line: 0>] - Initializing Gemini client...\n",
            "2025-04-11 21:40:23,566 - INFO - [<cell line: 0>] - Gemini client initialized successfully.\n",
            "2025-04-11 21:40:23,567 - INFO - [<cell line: 0>] - Locating external tools (Ghostscript, PDFtk)...\n",
            "2025-04-11 21:40:23,568 - INFO - [find_executable] - Searching for Ghostscript executable (checking names: ['gs'])...\n",
            "2025-04-11 21:40:23,571 - WARNING - [find_executable] - Ghostscript not found in PATH using names: ['gs'].\n",
            "2025-04-11 21:40:23,572 - WARNING - [find_executable] - Attempting to install Ghostscript (ghostscript) via apt-get...\n",
            "2025-04-11 21:40:51,027 - INFO - [find_executable] - Installation command for ghostscript completed.\n",
            "2025-04-11 21:40:51,029 - INFO - [find_executable] - Ghostscript installed and found at: /usr/bin/gs\n",
            "2025-04-11 21:40:51,031 - INFO - [find_executable] - Searching for PDFtk executable (checking names: ['pdftk'])...\n",
            "2025-04-11 21:40:51,032 - WARNING - [find_executable] - PDFtk not found in PATH using names: ['pdftk'].\n",
            "2025-04-11 21:40:51,033 - WARNING - [find_executable] - Attempting to install PDFtk (pdftk-java) via apt-get...\n",
            "2025-04-11 21:41:02,115 - INFO - [find_executable] - Installation command for pdftk-java completed.\n",
            "2025-04-11 21:41:02,116 - INFO - [find_executable] - PDFtk installed and found at: /usr/bin/pdftk\n",
            "2025-04-11 21:41:02,119 - INFO - [<cell line: 0>] - External tools located successfully.\n",
            "2025-04-11 21:41:02,121 - INFO - [<cell line: 0>] - Setup complete. Proceed to the next cell to upload the PDF.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### <font color=yellow>CELL 9: File Upload\n",
        "---\n",
        "This is where you'll upload a file (below this cell)."
      ],
      "metadata": {
        "id": "Vq3tpYmqmAjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cell 9: File Upload\n",
        "if __name__ == \"__main__\":\n",
        "    # Make sure setup in the previous cell succeeded (variables should exist)\n",
        "    if 'client' not in locals() or not client or not gs_path or not pdftk_path:\n",
        "         logging.error(\"Setup variables not found. Please run the previous cell successfully first.\")\n",
        "         raise RuntimeError(\"Setup cell did not complete successfully.\")\n",
        "\n",
        "    uploaded = None # Initialize uploaded variable\n",
        "    temp_upload_path = None # Initialize path variable\n",
        "\n",
        "    # --- 4. Handle File Upload via Colab ---\n",
        "    logging.info(\"=\"*20 + \" PDF Upload \" + \"=\"*20)\n",
        "    print(\"\\nPlease upload the single PDF file you want to process:\")\n",
        "    try:\n",
        "        uploaded = files.upload() # This widget will clear this cell's previous output\n",
        "    except Exception as upload_e:\n",
        "         logging.error(f\"An error occurred during file upload: {upload_e}\")\n",
        "         # Handle potential errors during the upload process itself\n",
        "\n",
        "    logging.info(\"=\"*52) # Separator after upload prompt finishes\n",
        "\n",
        "    # --- Validate and Save Upload ---\n",
        "    if uploaded:\n",
        "        if len(uploaded) > 1:\n",
        "            logging.error(\"Multiple files uploaded. Please run again and upload only one PDF.\")\n",
        "            # Clean up? Might be hard here. Best to just error out.\n",
        "            raise ValueError(\"Multiple files uploaded.\")\n",
        "        if len(uploaded) == 1:\n",
        "            uploaded_filename = list(uploaded.keys())[0]\n",
        "            uploaded_content = uploaded[uploaded_filename]\n",
        "\n",
        "            if not uploaded_filename.lower().endswith(\".pdf\"):\n",
        "                logging.error(f\"Uploaded file '{uploaded_filename}' is not a PDF.\")\n",
        "                raise ValueError(\"Uploaded file is not a PDF.\")\n",
        "            else:\n",
        "                # --- 5. Save Uploaded File Temporarily ---\n",
        "                try:\n",
        "                    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as temp_f:\n",
        "                        temp_f.write(uploaded_content)\n",
        "                        temp_upload_path = temp_f.name\n",
        "                    logging.info(f\"Uploaded file '{uploaded_filename}' ({len(uploaded_content)} bytes) saved temporarily to: {temp_upload_path}\")\n",
        "                    logging.info(\"Proceed to the next cell to process the file.\")\n",
        "                except Exception as save_e:\n",
        "                     logging.error(f\"Failed to save uploaded file temporarily: {save_e}\")\n",
        "                     temp_upload_path = None # Ensure path is None if save failed\n",
        "                     raise RuntimeError(\"Failed to save temporary file.\") from save_e\n",
        "        else: # uploaded is empty dict\n",
        "             logging.warning(\"No file was selected during upload.\")\n",
        "             # No need to raise error, just won't proceed\n",
        "    else:\n",
        "         logging.warning(\"File upload process did not return any files (cancelled or failed).\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# FILE UPLOAD BUTTON WILL APPEAR BELOW"
      ],
      "metadata": {
        "id": "_jDG-NMUqdMx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "fbb0247d-71b1-48d5-ffbf-e7d4cea0d900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-11 21:41:02,132 - INFO - [<cell line: 0>] - ==================== PDF Upload ====================\n",
            "\n",
            "Please upload the single PDF file you want to process:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d52c854a-6864-4628-b729-18fb48782d64\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d52c854a-6864-4628-b729-18fb48782d64\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving MSPTA_Drive Track_Combined Dwgs 125.pdf to MSPTA_Drive Track_Combined Dwgs 125.pdf\n",
            "2025-04-11 21:47:18,464 - INFO - [<cell line: 0>] - ====================================================\n",
            "2025-04-11 21:47:18,628 - INFO - [<cell line: 0>] - Uploaded file 'MSPTA_Drive Track_Combined Dwgs 125.pdf' (79104772 bytes) saved temporarily to: /tmp/tmpq6i2xdny.pdf\n",
            "2025-04-11 21:47:18,629 - INFO - [<cell line: 0>] - Proceed to the next cell to process the file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### <font color=yellow>CELL 10: Process Uploaded File and Cleanup\n",
        "---\n",
        "This cell \"calls\" the code from all of the previous cells where we set the project up."
      ],
      "metadata": {
        "id": "PD9kbUWwmiEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FILE UPLOAD BUTTON WILL APPEAR ABOVE\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "# Cell 10: Process Uploaded File and Cleanup\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if upload was successful and we have the path\n",
        "    if 'temp_upload_path' not in locals() or not temp_upload_path or not os.path.exists(temp_upload_path):\n",
        "         logging.error(\"Temporary upload path not found or file doesn't exist. Please run the upload cell successfully first.\")\n",
        "         # Don't raise error here, just skip processing if upload failed cleanly\n",
        "    elif 'client' not in locals() or not client or not gs_path or not pdftk_path:\n",
        "         logging.error(\"Setup variables not found. Please ensure Cell 8 ran successfully.\")\n",
        "         # Don't raise error here, just skip processing\n",
        "    else:\n",
        "        # Proceed with processing\n",
        "        processing_successful = False # Flag to track outcome\n",
        "        try:\n",
        "            # --- 6. Call the Handler Function ---\n",
        "            # Use uploaded_filename from the previous cell if you need it for logging\n",
        "            # If not, os.path.basename(temp_upload_path) will be the temp name\n",
        "            logging.info(f\"--- Calling processing handler for temporary file: {temp_upload_path} ---\")\n",
        "            processing_start_time = time.time()\n",
        "\n",
        "            final_output_file_path = handle_uploaded_pdf(\n",
        "                uploaded_pdf_path=temp_upload_path,\n",
        "                config=config,\n",
        "                gs_path=gs_path,\n",
        "                pdftk_path=pdftk_path,\n",
        "                client=client\n",
        "            )\n",
        "\n",
        "            processing_end_time = time.time()\n",
        "            processing_duration = processing_end_time - processing_start_time\n",
        "            logging.info(f\"--- Processing handler finished in {processing_duration:.2f} seconds ---\")\n",
        "\n",
        "            # --- 7. Report Final Status ---\n",
        "            if final_output_file_path:\n",
        "                logging.info(f\"=== Processing SUCCEEDED ===\")\n",
        "                logging.info(f\"Final bookmarked file saved to: {final_output_file_path}\")\n",
        "                processing_successful = True\n",
        "            else:\n",
        "                logging.error(f\"=== Processing FAILED ===\")\n",
        "                logging.error(\"Check logs above for specific error details.\")\n",
        "\n",
        "        except Exception as process_e:\n",
        "             logging.critical(f\"An unexpected critical error occurred during processing: {process_e}\", exc_info=True)\n",
        "\n",
        "        finally:\n",
        "            # --- 8. Cleanup ---\n",
        "            logging.info(\"--- Initiating final cleanup ---\")\n",
        "            if temp_upload_path and os.path.exists(temp_upload_path):\n",
        "                try:\n",
        "                    os.remove(temp_upload_path)\n",
        "                    logging.info(f\"Cleaned up temporary upload file: {temp_upload_path}\")\n",
        "                except OSError as e:\n",
        "                    logging.error(f\"Failed to clean up temporary upload file {temp_upload_path}: {e}\")\n",
        "            else:\n",
        "                 logging.info(\"No temporary upload file path found or file already deleted.\")\n",
        "\n",
        "    logging.info(\"=== Main Execution Block Finished ===\")"
      ],
      "metadata": {
        "id": "ZXqKiTQwqdMx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e8d52b4-7dea-4340-b395-07f5efac35ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-11 21:47:18,665 - INFO - [<cell line: 0>] - --- Calling processing handler for temporary file: /tmp/tmpq6i2xdny.pdf ---\n",
            "2025-04-11 21:47:18,668 - INFO - [handle_uploaded_pdf] - --- Starting processing request for file: tmpq6i2xdny.pdf ---\n",
            "2025-04-11 21:47:20,115 - INFO - [get_processing_paths] - Created temporary processing directory for intermediates: /content/drive/MyDrive/Apr11-Sheet-Extraction/processing_tmpq6i2xdny_a0112a48\n",
            "2025-04-11 21:47:20,116 - INFO - [get_processing_paths] - Generated processing paths for 'tmpq6i2xdny.pdf'.\n",
            "2025-04-11 21:47:20,118 - INFO - [process_pdf_workflow] - Starting PDF processing workflow for: tmpq6i2xdny.pdf\n",
            "2025-04-11 21:47:20,119 - INFO - [process_pdf_workflow] - Intermediate files will be stored in: /content/drive/MyDrive/Apr11-Sheet-Extraction/processing_tmpq6i2xdny_a0112a48\n",
            "2025-04-11 21:47:20,120 - INFO - [process_pdf_workflow] - Final output target: /content/drive/MyDrive/Apr11-Sheet-Extraction/Autobookmarked_tmpq6i2xdny.pdf\n",
            "2025-04-11 21:47:20,120 - INFO - [pdf_to_images] - Attempting PDF to image conversion for: tmpq6i2xdny.pdf\n",
            "2025-04-11 21:47:20,127 - INFO - [pdf_to_images] - Running Ghostscript command: /usr/bin/gs -dNOPAUSE -dBATCH -dSAFER -q -sDEVICE=pnggray -r120 -sOutputFile=/content/drive/MyDrive/Apr11-Sheet-Extraction/processing_tmpq6i2xdny_a0112a48/images/page_%04d.png /tmp/tmpq6i2xdny.pdf\n",
            "2025-04-11 21:49:46,381 - INFO - [pdf_to_images] - Ghostscript conversion command executed successfully.\n",
            "2025-04-11 21:49:46,388 - INFO - [pdf_to_images] - Found and sorted 125 image file(s) in /content/drive/MyDrive/Apr11-Sheet-Extraction/processing_tmpq6i2xdny_a0112a48/images.\n",
            "2025-04-11 21:49:46,389 - INFO - [generate_bookmarks_ai] - Starting AI bookmark generation for 125 images.\n",
            "2025-04-11 21:49:47,521 - INFO - [generate_bookmarks_ai] - Prepared 125 image parts for API.\n",
            "2025-04-11 21:49:47,527 - INFO - [generate_bookmarks_ai] - Calling Gemini model 'gemini-2.5-pro-exp-03-25' with 125 images...\n",
            "2025-04-11 21:49:47,528 - INFO - [generate_content] - AFC is enabled with max remote calls: 10.\n",
            "2025-04-11 21:50:48,268 - INFO - [_send_single_request] - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-exp-03-25:generateContent \"HTTP/1.1 200 OK\"\n",
            "2025-04-11 21:50:48,284 - INFO - [generate_content] - AFC remote call 1 is done.\n",
            "2025-04-11 21:50:48,285 - INFO - [generate_bookmarks_ai] - Gemini API call successful, attempting to parse JSON response.\n",
            "2025-04-11 21:50:48,286 - INFO - [generate_bookmarks_ai] - Successfully parsed JSON response from Gemini.\n",
            "2025-04-11 21:50:48,296 - INFO - [process_pdf_workflow] - Saved raw AI response JSON to /content/drive/MyDrive/Apr11-Sheet-Extraction/processing_tmpq6i2xdny_a0112a48/tmpq6i2xdny_response.json\n",
            "2025-04-11 21:50:48,297 - INFO - [convert_ai_response_to_pdftk] - Converting AI response dictionary to PDFtk format for: /content/drive/MyDrive/Apr11-Sheet-Extraction/processing_tmpq6i2xdny_a0112a48/tmpq6i2xdny_bookmarks.txt\n",
            "2025-04-11 21:50:48,306 - INFO - [convert_ai_response_to_pdftk] - Successfully saved PDFtk bookmarks (125 entries) to /content/drive/MyDrive/Apr11-Sheet-Extraction/processing_tmpq6i2xdny_a0112a48/tmpq6i2xdny_bookmarks.txt\n",
            "2025-04-11 21:50:48,309 - INFO - [apply_bookmarks] - Applying bookmarks from tmpq6i2xdny_bookmarks.txt to tmpq6i2xdny.pdf\n",
            "2025-04-11 21:50:48,310 - INFO - [apply_bookmarks] - Running PDFtk command: /usr/bin/pdftk /tmp/tmpq6i2xdny.pdf update_info /content/drive/MyDrive/Apr11-Sheet-Extraction/processing_tmpq6i2xdny_a0112a48/tmpq6i2xdny_bookmarks.txt output /content/drive/MyDrive/Apr11-Sheet-Extraction/Autobookmarked_tmpq6i2xdny.pdf\n",
            "2025-04-11 21:50:52,094 - INFO - [apply_bookmarks] - PDFtk bookmark application successful. Output saved to: /content/drive/MyDrive/Apr11-Sheet-Extraction/Autobookmarked_tmpq6i2xdny.pdf\n",
            "2025-04-11 21:50:52,095 - INFO - [process_pdf_workflow] - Workflow completed successfully for tmpq6i2xdny.pdf.\n",
            "2025-04-11 21:50:52,096 - INFO - [process_pdf_workflow] - Initiating cleanup for temporary directory: /content/drive/MyDrive/Apr11-Sheet-Extraction/processing_tmpq6i2xdny_a0112a48\n",
            "2025-04-11 21:50:52,352 - INFO - [process_pdf_workflow] - Successfully cleaned up temporary directory: /content/drive/MyDrive/Apr11-Sheet-Extraction/processing_tmpq6i2xdny_a0112a48\n",
            "2025-04-11 21:50:52,353 - INFO - [handle_uploaded_pdf] - Successfully processed 'tmpq6i2xdny.pdf'.\n",
            "2025-04-11 21:50:52,354 - INFO - [<cell line: 0>] - --- Processing handler finished in 213.69 seconds ---\n",
            "2025-04-11 21:50:52,355 - INFO - [<cell line: 0>] - === Processing SUCCEEDED ===\n",
            "2025-04-11 21:50:52,356 - INFO - [<cell line: 0>] - Final bookmarked file saved to: /content/drive/MyDrive/Apr11-Sheet-Extraction/Autobookmarked_tmpq6i2xdny.pdf\n",
            "2025-04-11 21:50:52,357 - INFO - [<cell line: 0>] - --- Initiating final cleanup ---\n",
            "2025-04-11 21:50:52,380 - INFO - [<cell line: 0>] - Cleaned up temporary upload file: /tmp/tmpq6i2xdny.pdf\n",
            "2025-04-11 21:50:52,381 - INFO - [<cell line: 0>] - === Main Execution Block Finished ===\n"
          ]
        }
      ]
    }
  ]
}